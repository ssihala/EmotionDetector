{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAI4104 Final Project: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 16:56:54.513190: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-16 16:56:54.559110: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-16 16:56:54.559140: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-16 16:56:54.560820: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-16 16:56:54.572208: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-16 16:56:54.572872: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-16 16:56:55.557083: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "### Python version: 3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]\n",
      "### NumPy version: 1.24.3\n",
      "### Scikit-learn version: 1.3.0\n",
      "### Tensorflow version: 2.15.0\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "# Uncomment this to install opencv\n",
    "#%conda install conda-forge::opencv\n",
    "import os\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "import utils\n",
    "\n",
    "# Let's check our software versions\n",
    "print('------------')\n",
    "print('### Python version: ' + __import__('sys').version)\n",
    "print(f'### NumPy version: {np.__version__}')\n",
    "print(f'### Scikit-learn version: {sklearn.__version__}')\n",
    "print(f'### Tensorflow version: {tf.__version__}')\n",
    "print('------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/ananthu017/emotion-detection-fer\n",
      "License(s): CC0-1.0\n",
      "Downloading emotion-detection-fer.zip to /home/sihala/EmotionDetector/data\n",
      "100%|█████████████████████████████████████▉| 65.0M/65.2M [00:26<00:00, 2.51MB/s]\n",
      "100%|██████████████████████████████████████| 65.2M/65.2M [00:27<00:00, 2.53MB/s]\n"
     ]
    }
   ],
   "source": [
    "kaggle_token_path = os.path.dirname(os.getcwd())+\"/token\"\n",
    "data_path = os.path.dirname(os.getcwd())+\"/data\"\n",
    "os.environ[\"KAGGLE_CONFIG_DIR\"] = kaggle_token_path\n",
    "os.chmod(kaggle_token_path+\"/kaggle.json\", 0o600)\n",
    "\n",
    "import kaggle\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "os.chdir(data_path)\n",
    "!kaggle datasets download ananthu017/emotion-detection-fer --unzip\n",
    "os.chdir(current_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File to nparray conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative path to data\n",
    "data_path = '../data/'\n",
    "\n",
    "# Category names (implicitly stores labels as indices)\n",
    "categories = ['angry', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
    "num_categories = len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from ../data/test/fearful    [#########################] \n",
      "Loading images from ../data/test/sad        [#########################] \n",
      "Loading images from ../data/test/disgusted  [#########################] \n",
      "Loading images from ../data/test/neutral    [#########################] \n",
      "Loading images from ../data/test/angry      [#########################] \n",
      "Loading images from ../data/test/happy      [#########################] \n",
      "Loading images from ../data/test/surprised  [#########################] \n",
      "Loading images from ../data/train/fearful   [#########################] \n",
      "Loading images from ../data/train/sad       [#########################] \n",
      "Loading images from ../data/train/disgusted [#########################] \n",
      "Loading images from ../data/train/neutral   [#########################] \n",
      "Loading images from ../data/train/angry     [#########################] \n",
      "Loading images from ../data/train/happy     [#########################] \n",
      "Loading images from ../data/train/surprised [#########################] \n",
      "Loaded all images.\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "label_list = []\n",
    "\n",
    "\n",
    "for (root, dirs, files) in os.walk(data_path):\n",
    "    # Get top directory name\n",
    "    last_slash = root.rfind('/') + 1\n",
    "    dir_name = root[last_slash:]\n",
    "\n",
    "    # Get associated label\n",
    "    label = -1\n",
    "    try:\n",
    "        label = categories.index(dir_name)\n",
    "    except ValueError:\n",
    "        None\n",
    "\n",
    "    # Not an image directory\n",
    "    if label == -1:\n",
    "        continue\n",
    "    \n",
    "    # Progress bar reset\n",
    "    bar = utils.ProgressBar(25, f'Loading images from {root:24}')\n",
    "    num_imgs = len(files)\n",
    "\n",
    "    # Add all files for current dir\n",
    "    for i, f in enumerate(files):\n",
    "        # Load image data\n",
    "        filepath = root+'/'+f\n",
    "        img = cv2.imread(filepath)\n",
    "        \n",
    "        # Add to list\n",
    "        data_list.append(img)\n",
    "        label_list.append(label)\n",
    "        # Update progress bar\n",
    "        bar.update_display((i + 1) / num_imgs)\n",
    "\n",
    "\n",
    "print('Loaded all images.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting and One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35887, 48, 48, 1), (35887, 7))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert data to np arrays and sort\n",
    "all_t_num = np.array(label_list)\n",
    "resort_inds = np.argsort(all_t_num)\n",
    "all_t_num = all_t_num[resort_inds]\n",
    "\n",
    "all_x_bgr = np.array(data_list)\n",
    "all_x = np.average(all_x_bgr, axis=3)[resort_inds]\n",
    "all_x = all_x.reshape(*all_x.shape, 1)\n",
    "\n",
    "# One-hot encoding\n",
    "all_t = keras.utils.to_categorical(all_t_num, num_classes=num_categories)\n",
    "\n",
    "num_images = all_x.shape[0]\n",
    "image_shape = all_x.shape[1:]\n",
    "\n",
    "all_x.shape, all_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data scaling and partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Data\n",
      "------------------------------------\n",
      "Training Data:   (25120, 48, 48, 1)\n",
      "Validation Data: (5383, 48, 48, 1)\n",
      "Test Data:       (5384, 48, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Train, validation, test proportions\n",
    "prop_vec = np.array([0.7, 0.15, 0.15])\n",
    "seed = 42\n",
    "\n",
    "# Flattened data\n",
    "all_x_flat = all_x.reshape(all_x.shape[0], image_shape[0]*image_shape[1])\n",
    "\n",
    "# Train-other split\n",
    "train_prop = prop_vec[0]\n",
    "train_x_unscaled, tmp_x_unscaled, train_t, tmp_t = train_test_split(all_x_flat, all_t, train_size=train_prop, random_state=seed)\n",
    "\n",
    "#! Special image case? Should we just scale fitted to all data because we know all features lie on 0-255?\n",
    "# Fit only on training\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(all_x_flat)\n",
    "# Transform both splits\n",
    "train_x_flat = scaler.transform(train_x_unscaled)\n",
    "tmp_x_flat = scaler.transform(tmp_x_unscaled)\n",
    "\n",
    "# Validation-test split\n",
    "val_prop = prop_vec[1] / (1 - train_prop)\n",
    "val_x_flat, test_x_flat, val_t, test_t = train_test_split(tmp_x_flat, tmp_t, train_size=val_prop, random_state=seed)\n",
    "\n",
    "# Reshape into images\n",
    "train_x = train_x_flat.reshape(train_x_flat.shape[0], *image_shape)\n",
    "val_x = val_x_flat.reshape(val_x_flat.shape[0], *image_shape)\n",
    "test_x = test_x_flat.reshape(test_x_flat.shape[0], *image_shape)\n",
    "\n",
    "print('Image Data')\n",
    "print('-'*36)\n",
    "print(f'Training Data:   {train_x.shape}')\n",
    "print(f'Validation Data: {val_x.shape}')\n",
    "print(f'Test Data:       {test_x.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save split data\n",
    "np.savez_compressed(data_path+'data', \n",
    "                    train_x=train_x, train_t=train_t, \n",
    "                    val_x=val_x, val_t=val_t, \n",
    "                    test_x=test_x, test_t=test_t\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def augmentations(images, labels, model, iters=3):\n",
    "    image_list = []\n",
    "    cat_list = []\n",
    "\n",
    "    bar = utils.ProgressBar(50, 'Generating Augmented Images: ')\n",
    "\n",
    "    for i, (img, cat) in enumerate(zip(images, labels)):\n",
    "        frac = (i + 1) / labels.shape[0]\n",
    "        for a in range(iters):\n",
    "            image_list.append(model(img))\n",
    "            cat_list.append(cat)\n",
    "        bar.update_display(frac, f'{frac:.2%}')\n",
    "\n",
    "    return np.array(image_list), np.array(cat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Augmented Images: [##################################################] 100.00%\n"
     ]
    }
   ],
   "source": [
    "augment = keras.Sequential([\n",
    "    keras.layers.RandomFlip('horizontal'), \n",
    "    keras.layers.RandomRotation(0.05), \n",
    "    keras.layers.RandomZoom(height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2)), \n",
    "    keras.layers.RandomContrast(factor=0.2)\n",
    "])\n",
    "\n",
    "n_augmentations = 5\n",
    "train_x_aug, train_t_aug = augmentations(train_x, train_t, augment, n_augmentations)\n",
    "\n",
    "np.savez_compressed(data_path+'data_aug', train_x=train_x_aug, train_t=train_t_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Tensorflow-2.7.0",
   "language": "python",
   "name": "tensorflow-2.7.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
