{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb79b28-55e1-41bf-ba78-2e4c35855526",
   "metadata": {},
   "source": [
    "# CAI4104 Final Project: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ccbe021-95a9-42d3-a6b3-02f319055f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 16:47:46.950156: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-21 16:47:46.953422: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-21 16:47:47.080707: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-21 16:47:47.472316: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-21 16:47:49.033414: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "### Python version: 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:23:14) [GCC 10.4.0]\n",
      "### NumPy version: 1.26.3\n",
      "### SciPy version: 1.11.4\n",
      "### Scikit-learn version: 1.3.0\n",
      "### Tensorflow version: 2.16.1\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import sklearn\n",
    "import utils\n",
    "\n",
    "# Let's check our software versions\n",
    "print('------------')\n",
    "print('### Python version: ' + __import__('sys').version)\n",
    "print(f'### NumPy version: {np.__version__}')\n",
    "print(f'### SciPy version: {sp.__version__}')\n",
    "print(f'### Scikit-learn version: {sklearn.__version__}')\n",
    "print(f'### Tensorflow version: {tf.__version__}')\n",
    "print('------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29486eba-fb6d-4891-bd0c-511666e4abd1",
   "metadata": {},
   "source": [
    "# Loading the models and model histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21ba8a6-de5d-4b3f-a668-0db22fbe77a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_path = '../predicted/'\n",
    "histories_path = '../histories/'\n",
    "\n",
    "test_t_dummy_pred = np.load(predicted_path + 'test_t_dummy_pred.npy')\n",
    "test_t_nb_pred = np.load(predicted_path + 'test_t_nb_pred.npy')\n",
    "test_t_lr_pred = np.load(predicted_path + 'test_t_lr_pred.npy')\n",
    "test_t_cnn_pred = np.load(predicted_path + 'test_t_cnn_pred.npy')\n",
    "\n",
    "dummy_hist = np.load(histories_path + 'dummy_hist.npy')\n",
    "nb_hist = np.load(histories_path + 'nb_hist.npy')\n",
    "lr_hist = np.load(histories_path + 'lr_hist.npy')\n",
    "cnn_hist = np.load(histories_path + 'cnn_hist.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9193282-8b7e-4ec9-83a8-8d37182320a7",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27eb4d77-ee11-4295-8f69-5011ecb9bff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35110 images with shape (48, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "# Relative path to .npy files\n",
    "data_path = '../data/'\n",
    "data = np.load(data_path + 'data.npz')\n",
    "\n",
    "# Load numpy arrays\n",
    "train_x = data['train_x']\n",
    "train_t = data['train_t']\n",
    "\n",
    "val_x = data['val_x']\n",
    "val_t = data['val_t']\n",
    "\n",
    "test_x = data['test_x']\n",
    "test_t = data['test_t']\n",
    "\n",
    "assert train_x.shape[0] == train_t.shape[0], \"Training image quantity mismatches label quantity\"\n",
    "assert val_x.shape[0] == val_t.shape[0], \"Validation image quantity mismatches label quantity\"\n",
    "assert test_x.shape[0] == test_t.shape[0], \"Test image quantity mismatches label quantity\"\n",
    "\n",
    "num_images = train_x.shape[0] + val_x.shape[0] + test_x.shape[0]\n",
    "image_shape = train_x.shape[1:]\n",
    "\n",
    "print(f'{num_images} images with shape {image_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a24dff9-dfcc-4430-a80f-23e8144a2011",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_flat = train_x.reshape(train_x.shape[0], train_x.shape[1]*train_x.shape[2])\n",
    "val_x_flat = val_x.reshape(val_x.shape[0], val_x.shape[1]*val_x.shape[2])\n",
    "test_x_flat = test_x.reshape(test_x.shape[0], test_x.shape[1]*test_x.shape[2])\n",
    "\n",
    "train_t_num = np.array([np.argmax(a) for a in train_t])\n",
    "val_t_num = np.array([np.argmax(a) for a in val_t])\n",
    "test_t_num = np.array([np.argmax(a) for a in test_t])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88cc389-96e5-4f3d-a29e-1e40dc08c22a",
   "metadata": {},
   "source": [
    "# Compute metrics for each model on test set\n",
    "## Metrics:\n",
    "####     - Accuracy\n",
    "####     - F1 Score\n",
    "####     - Precision\n",
    "####     - Recall\n",
    "####     - Area Under Curve (AUC), Receiver Operating Characteristic (ROC), done per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a99bf40b-d686-488d-952b-1d9945069d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.metrics import Accuracy, F1Score, Precision, Recall\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def compute_metrics (name, test_t_pred):\n",
    "    metrics = {}\n",
    "\n",
    "    acc = Accuracy()\n",
    "    acc.update_state(test_t, test_t_pred)\n",
    "    metrics['accuracy'] = acc.result()\n",
    "\n",
    "    f1 = F1Score()\n",
    "    f1.update_state(test_t, test_t_pred)\n",
    "    metrics['f1score'] = f1.result()\n",
    "\n",
    "    prec = Precision()\n",
    "    prec.update_state(test_t, test_t_pred)\n",
    "    metrics['precision'] = prec.result()\n",
    "\n",
    "    rec = Recall()\n",
    "    rec.update_state(test_t, test_t_pred)\n",
    "    metrics['recall'] = rec.result()\n",
    "\n",
    "    print('{}:\\n Accuracy: {:.2f}, F1-Score: {:.2f}, Precision: {:.2f}, Recall: {:.2f}'.format(\n",
    "        name, metrics['accuracy'], metrics['f1score'], metrics['precision'], metrics['recall']\n",
    "    ))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def compute_roc (test_t_pred):\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc = {}\n",
    "    \n",
    "    t_max = np.max(test_t_num)\n",
    "    for i in range(t_max):\n",
    "        fpr[i], tpr[i], _ = roc_curve(test_t[:, i], test_t_pred[:, i])\n",
    "        roc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    return fpr, tpr, roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33b849-57a8-4f67-896b-a35368df4d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_metrics = compute_metrics('Dummy Classifier', test_t_dummy_pred)\n",
    "dummy_fpr, dummy_tpr, dummy_roc = compute_roc(test_t_dummy_pred)\n",
    "\n",
    "nb_metrics = compute_metrics('Naive Bayes', test_t_nb_pred)\n",
    "nb_fpr, nb_tpr, nb_roc = compute_roc(test_t_nb_pred)\n",
    "\n",
    "lr_metrics = compute_metrics('Logistic Regression', test_t_lr_pred)\n",
    "lr_fpr, lr_tpr, lr_roc = compute_roc(test_t_lr_pred)\n",
    "\n",
    "cnn_metrics = compute_metrics('CNN', test_t_cnn_pred)\n",
    "cnn_fpr, cnn_tpr, cnn_roc = compute_roc(test_t_cnn_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efa863e-b623-47f7-b20a-ef8143cb8867",
   "metadata": {},
   "source": [
    "# Evaluate and create graphs for each model history and ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9edeb696-d153-4999-92b3-196be8f824c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_history (name, history, metrics):\n",
    "    for metric in metrics.keys():\n",
    "        plt.plot(history.history[metric])\n",
    "        plt.plot(history.history['val_' + metric])\n",
    "        plt.axhline(metrics[metric])\n",
    "        plt.title('{} {}'.format(name, metric))\n",
    "        plt.ylabel(metric)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val', 'test'], loc='upper left')\n",
    "        plt.show()   \n",
    "\n",
    "def evaluate_roc (name, fpr, tpr, roc):\n",
    "    colors = ['red', 'green', 'purple', 'yellow', 'grey', 'blue', 'orange']\n",
    "    classes = ['angry', 'disgusted', 'fearful', 'happy', 'sad', 'surprised']\n",
    "    for i, color in enumerate(colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw, label='ROC of class {} (area = {1:0.2f})'.format(classes[i], roc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('false positive rate')\n",
    "    plt.ylabel('true positive rate')\n",
    "    plt.title('receiver operating characteristic on test set for each class of model {}'.format(name))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b6b247-4980-49a5-b17e-57903727d4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_history('Dummy Classifier', dummy_hist, dummy_metrics)\n",
    "evaluate_roc('Dummy Classifier', dummy_fpr, dummy_tpr, dummy_roc)\n",
    "\n",
    "evaluate_history('Naive Bayes', nb_hist, nb_metrics)\n",
    "evaluate_roc('Naive Bayes', nb_fpr, nb_tpr, nb_roc)\n",
    "\n",
    "evaluate_history('Logistic Regression', lr_hist, lr_metrics)\n",
    "evaluate_roc('Logistic Regression', lr_fpr, lr_tpr, lr_roc)\n",
    "\n",
    "evaluate_history('CNN', cnn_hist, cnn_metrics)\n",
    "evaluate_roc('CNN', cnn_fpr, cnn_tpr, cnn_roc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
