{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb79b28-55e1-41bf-ba78-2e4c35855526",
   "metadata": {},
   "source": [
    "# CAI4104 Final Project: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ccbe021-95a9-42d3-a6b3-02f319055f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 15:01:39.467094: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-20 15:01:39.469751: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-20 15:01:39.472950: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-20 15:01:39.511335: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-20 15:01:40.542129: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "### Python version: 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:23:14) [GCC 10.4.0]\n",
      "### NumPy version: 1.26.3\n",
      "### SciPy version: 1.11.4\n",
      "### Scikit-learn version: 1.3.0\n",
      "### Tensorflow version: 2.16.1\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import sklearn\n",
    "import utils\n",
    "\n",
    "# Let's check our software versions\n",
    "print('------------')\n",
    "print('### Python version: ' + __import__('sys').version)\n",
    "print(f'### NumPy version: {np.__version__}')\n",
    "print(f'### SciPy version: {sp.__version__}')\n",
    "print(f'### Scikit-learn version: {sklearn.__version__}')\n",
    "print(f'### Tensorflow version: {tf.__version__}')\n",
    "print('------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29486eba-fb6d-4891-bd0c-511666e4abd1",
   "metadata": {},
   "source": [
    "# Loading the models and model histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21ba8a6-de5d-4b3f-a668-0db22fbe77a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../model/'\n",
    "history_path = '../history/'\n",
    "\n",
    "dummy_model = \n",
    "nb_model = \n",
    "lr_model = \n",
    "cnn_model = \n",
    "\n",
    "dummy_hist = np.load(history_path + 'dummy_hist.npy')\n",
    "nb_hist = np.load(history_path + 'nb_hist.npy')\n",
    "lr_hist = np.load(history_path + 'lr_hist.npy')\n",
    "cnn_hist = np.load(history_path + 'cnn_hist.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9193282-8b7e-4ec9-83a8-8d37182320a7",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27eb4d77-ee11-4295-8f69-5011ecb9bff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35110 images with shape (48, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "# Relative path to .npy files\n",
    "data_path = '../data/'\n",
    "data = np.load(data_path + 'data.npz')\n",
    "\n",
    "# Load numpy arrays\n",
    "train_x = data['train_x']\n",
    "train_t = data['train_t']\n",
    "\n",
    "val_x = data['val_x']\n",
    "val_t = data['val_t']\n",
    "\n",
    "test_x = data['test_x']\n",
    "test_t = data['test_t']\n",
    "\n",
    "assert train_x.shape[0] == train_t.shape[0], \"Training image quantity mismatches label quantity\"\n",
    "assert val_x.shape[0] == val_t.shape[0], \"Validation image quantity mismatches label quantity\"\n",
    "assert test_x.shape[0] == test_t.shape[0], \"Test image quantity mismatches label quantity\"\n",
    "\n",
    "num_images = train_x.shape[0] + val_x.shape[0] + test_x.shape[0]\n",
    "image_shape = train_x.shape[1:]\n",
    "\n",
    "print(f'{num_images} images with shape {image_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a24dff9-dfcc-4430-a80f-23e8144a2011",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_flat = train_x.reshape(train_x.shape[0], train_x.shape[1]*train_x.shape[2])\n",
    "val_x_flat = val_x.reshape(val_x.shape[0], val_x.shape[1]*val_x.shape[2])\n",
    "test_x_flat = test_x.reshape(test_x.shape[0], test_x.shape[1]*test_x.shape[2])\n",
    "\n",
    "train_t_num = np.array([np.argmax(a) for a in train_t])\n",
    "val_t_num = np.array([np.argmax(a) for a in val_t])\n",
    "test_t_num = np.array([np.argmax(a) for a in test_t])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88cc389-96e5-4f3d-a29e-1e40dc08c22a",
   "metadata": {},
   "source": [
    "# Evaluate each model on test set\n",
    "## Metrics:\n",
    "####     - Accuracy\n",
    "####     - F1 Score\n",
    "####     - Precision\n",
    "####     - Recall\n",
    "####     - Area Under Curve (AUC), done per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a99bf40b-d686-488d-952b-1d9945069d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics (name, model):\n",
    "    metrics = {}\n",
    "    test_t_pred = model.predict(test_t)\n",
    "\n",
    "    acc = Accuracy()\n",
    "    acc.update_state(test_t, test_t_pred)\n",
    "    metrics['accuracy'] = acc.result()\n",
    "\n",
    "    f1 = F1Score()\n",
    "    f1.update_state(test_t, test_t_pred)\n",
    "    metrics['f1score'] = f1.result()\n",
    "\n",
    "    prec = Precision()\n",
    "    prec.update_state(test_t, test_t_pred)\n",
    "    metrics['precision'] = prec.result()\n",
    "\n",
    "    rec = Recall()\n",
    "    rec.update_state(test_t, test_t_pred)\n",
    "    metrics['recall'] = rec.result()\n",
    "\n",
    "    t_max = np.max(test_t_num)\n",
    "    for i in range(t_max):\n",
    "        mc_auc = MulticlassAUC(t_label=i)\n",
    "        mc_auc.update_state(test_t, test_t_pred)\n",
    "\n",
    "        if (i == 0):\n",
    "            metrics['auc']['angry'] = mc_auc.result()\n",
    "        elif (i == 1):\n",
    "            metrics['auc']['disgusted'] = mc_auc.result()\n",
    "        elif (i == 2):\n",
    "            metrics['auc']['fearful'] = mc_auc.result()\n",
    "        elif (i == 3):\n",
    "            metrics['auc']['happy'] = mc_auc.result()\n",
    "        elif (i == 4):\n",
    "            metrics['auc']['neutral'] = mc_auc.result()\n",
    "        elif (i == 5):\n",
    "            metrics['auc']['sad'] = mc_auc.result()\n",
    "        elif (i == 6):\n",
    "            metrics['auc']['surprised'] = mc_auc.result()\n",
    "\n",
    "    print('{}:\\n Accuracy: {:.2f}, F1-Score: {:.2f}, Precision: {:.2f}, Recall: {:.2f}, AUC:\\n\\tAngry: {:.2f}\\n\\tDisgusted: {:.2f}\\n\\tFearful: {:.2f}\\n\\tHappy: {:.2f}\\n\\tNeutral: {:.2f}\\n\\tSad: {:.2f}\\n\\tSurprised: {:.2f}'.format(\n",
    "        name, metrics['accuracy'], metrics['f1score'], metrics['precision'], metrics['recall'], \n",
    "        metrics['auc']['angry'], metrics['auc']['disgusted'], metrics['auc']['fearful'], metrics['auc']['happy'], \n",
    "        metrics['auc']['neutral'], metrics['auc']['sad'], metrics['auc']['surprised'] \n",
    "    ))\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33b849-57a8-4f67-896b-a35368df4d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_metrics = evaluate_metrics('Dummy Classifier', dummy_model)\n",
    "nb_metrics = evaluate_metrics('Naive Bayes', nb_model)\n",
    "lr_metrics = evaluate_metrics('Logistic Regression', lr_model)\n",
    "cnn_metrics = evaluate_metrics('CNN', cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efa863e-b623-47f7-b20a-ef8143cb8867",
   "metadata": {},
   "source": [
    "# Evaluate each model history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3418321-a246-4c0e-9838-a418263e48a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_history (name, history, metrics):\n",
    "    for metric in metrics.keys():\n",
    "        if (metric == 'auc'):\n",
    "            # this is wrong for sure, roc is not epoch based, we shall fix\n",
    "            # use as basis: https://github.com/Tony607/ROC-Keras/blob/master/ROC-Keras.ipynb\n",
    "            for emotion in metrics['auc'].keys():\n",
    "                plt.plot(history.history['MulticlassAUC_' + emotion])\n",
    "                plt.plot(history.history['val_MulticlassAUC_' + emotion])\n",
    "                plt.axhline(metrics[metric][emotion])\n",
    "                plt.title('{} {} {}'.format(name, metric, emotion))\n",
    "                plt.ylabel(metric)\n",
    "                plt.xlabel('epoch')\n",
    "                plt.legend(['train', 'val', 'test'], loc='upper left')\n",
    "                plt.show() \n",
    "        else:\n",
    "            plt.plot(history.history[metric])\n",
    "            plt.plot(history.history['val_' + metric])\n",
    "            plt.axhline(metrics[metric])\n",
    "            plt.title('{} {}'.format(name, metric))\n",
    "            plt.ylabel(metric)\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'val', 'test'], loc='upper left')\n",
    "            plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10487999-c42b-4314-acca-2c3042e68fc6",
   "metadata": {},
   "source": [
    "# Print associated graphs for comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07a825b0-b547-41c2-a977-4304c557706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look all dis shiz up bc i aint got a clue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b6b247-4980-49a5-b17e-57903727d4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_history('Dummy Classifier', dummy_hist, dummy_metrics)\n",
    "evaluate_history('Naive Bayes', nb_hist, nb_metrics)\n",
    "evaluate_history('Logistic Regression', lr_hist, lr_metrics)\n",
    "evaluate_history('CNN', cnn_hist, cnn_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
